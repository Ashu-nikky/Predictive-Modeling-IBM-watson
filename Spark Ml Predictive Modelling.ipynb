{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "#Install the wget which is used to get the data from web servers\n!pip install wget --user ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Importing the NAIVEbayes go sales data\nimport wget\n                        \nlink_to_data = 'https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600'\nfilename = wget.download(link_to_data)\n\nprint filename", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# spark 2.2.0 intialization \n\nspark = SparkSession.builder.getOrCreate()\n\n#Dataframe\n\ndf_data = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n        .option('header','true').option('inferSchema','true').load(filename)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_data.printSchema()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_data.show(10,False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Total count for the input GO Sales dataset is \"+ str(df_data.count()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Apache\u00ae Spark machine learning model\n\n#Prepare data randomSplit(weights, seed=None)\n#Parameters:\t\n#weights \u2013 list of doubles as weights with which to split the DataFrame. Weights will be normalized if they don\u2019t sum up to 1.0.\n#seed \u2013 The seed for sampling.\n\nsplitted_data = df_data.randomSplit([0.8, 0.18, 0.02], 24)\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\npredict_data = splitted_data[2]\n\nprint \"Number of training records: \" + str(train_data.count())\n#train_data.show(10, False)\nprint \"Number of testing records : \" + str(test_data.count())\n#test_data.show(10, False)\nprint \"Number of prediction records : \" + str(predict_data.count())\n#predict_data.show(10, False)\nprint \"total count of the dataset: \" + str(predict_data.count() + train_data.count() + test_data.count())\n\n#The train data set, which is the largest group, is used for training.\n#The test data set will be used for model evaluation and is used to test the assumptions of the model.\n#The predict data set will be used for prediction.", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Create pipeline and train a model\n\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Convert all the string fields to numeric ones by using the StringIndexer transformer.\n#indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_data) for column in list(set(df_data.columns)) ]\n\nstringIndexer_label = StringIndexer(inputCol=\"PRODUCT_LINE\", outputCol=\"label\").fit(df_data)\nstringIndexer_prof = StringIndexer(inputCol=\"PROFESSION\", outputCol=\"PROFESSION_IX\")\nstringIndexer_gend = StringIndexer(inputCol=\"GENDER\", outputCol=\"GENDER_IX\")\nstringIndexer_mar = StringIndexer(inputCol=\"MARITAL_STATUS\", outputCol=\"MARITAL_STATUS_IX\")\n\n#pipeline = Pipeline(stages=indexers)\n#df_r = pipeline.fit(df_data).transform(df_data)\n#df_r.show(10, False)\n#df_ind = stringIndexer_label.transform(df_data)\n#df_ind.printSchema()\n#df_ind.show(50, False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Create a feature vector by combining all features together..\nvectorAssembler_features = VectorAssembler(inputCols=[\"GENDER_IX\", \"AGE\", \"MARITAL_STATUS_IX\", \"PROFESSION_IX\"], outputCol=\"features\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#define estimators you want to use for classification\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#A pipeline consists of transformers and an estimator\npipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar,\n                               vectorAssembler_features, rf, labelConverter])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Training the Model\ntrain_data.printSchema()\nmodel_rf = pipeline_rf.fit(train_data)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#model accuracy \npredictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Persist model\n\nfrom repository_v3.mlrepositoryclient import MLRepositoryClient\nfrom repository_v3.mlrepositoryartifact import MLRepositoryArtifact", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "wml_credentials={\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"access_key\": \"***\",\n  \"username\": \"919bbb87-dce6-4218-839c-7d6ce418b987\",\n  \"password\": \"e10fa74e-8691-44f8-8e88-9fdf2013b1b8\",\n  \"instance_id\": \"028b5393-6057-48a7-a8f2-a5d49679ed4f\"\n}", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "ml_repository_client = MLRepositoryClient(wml_credentials['url'])\nml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pipeline_artifact = MLRepositoryArtifact(pipeline_rf, name=\"pipeline\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "model_artifact = MLRepositoryArtifact(model_rf, training_data=train_data, name=\"Product Line Prediction\", pipeline_artifact=pipeline_artifact)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Save pipeline and model\nsaved_model = ml_repository_client.models.save(model_artifact)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Get saved model metadata from Watson Machine Learning.\n#meta.available_props() to get the list of available props.\nsaved_model.meta.available_props()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print \"modelType: \" + saved_model.meta.prop(\"frameworkName\")\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"label_column: \" + str(saved_model.meta.prop(\"label_column\"))\nprint \"inputDataSchema: \" + str(saved_model.meta.prop(\"inputDataSchema\"))\nprint \"modelVersionUrl: \" + str(saved_model.meta.prop(\"modelVersionUrl\"))\nprint \"framework_runtimes: \" + str(saved_model.meta.prop(\"framework_runtimes\"))\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Load model\nloadedModelArtifact = ml_repository_client.models.get(saved_model.uid)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print str(loadedModelArtifact.name)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#Predict locally and visualize\n\npredictions = loadedModelArtifact.model_instance().transform(predict_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "predictions.show(5,False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show(truncate=False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}